## 1.视觉：

segment-anything（https://github.com/facebookresearch/segment-anything）：图像分割模型，支持拉框、点击、auto分割

SegGPT（https://github.com/baaivision/Painter/tree/main/SegGPT）：one-shot的图像分割模型，一触百通

OpenSeeD（https://github.com/IDEA-Research/OpenSeeD）：交互式的图像分割模型，输入2d框，生成掩码图片

## 2.文本：

LLaMA（https://github.com/facebookresearch/llama）：文本生成模型

PandaLM（https://github.com/WeOpenML/PandaLM）：是一个要语言评估模型

ColossalAI（https://github.com/hpcaitech/ColossalAI）：chatgpt类的文本对话模型

DeepSpeed-Chat（https://github.com/microsoft/DeepSpeedExamples）：chatgpt类的文本对话模型

## 3.多模态：

CLIP（https://github.com/openai/CLIP）：对比学习的视觉语言模型，可以对任意图像和文本进行encode，根据encode结果计算图像和文本的相似度，从而可以进行zero-shot的图像分类任务

ALBEF（https://github.com/salesforce/ALBEF）：视觉语言模型，支持特征提取、图像说明、图像文本匹配、文本位置检索、视觉问答、zero-shot图像分类

BLIP（https://github.com/salesforce/BLIP）：视觉语言模型，支持特征提取、图像说明、图像文本匹配、文本位置检索、视觉问答、zero-shot图像分类

BLIP2（https://github.com/salesforce/LAVIS/tree/main）：视觉语言模型，支持特征提取、图像说明、图像文本匹配、文本位置检索、视觉问答、zero-shot图像分类

Tag2Text（https://github.com/xinyu1205/Tag2Text）：可以根据图像生成图像说明和图像标签

Stable Diffusion（https://github.com/CompVis/stable-diffusion）：文生图的扩散模型

GroundingDINO（https://github.com/IDEA-Research/GroundingDINO）：根据prompt来对图像进行目标检测

Grounded-Segment-Anything（https://github.com/IDEA-Research/Grounded-Segment-Anything）：根据prompt来对图像进行目标检测和实例分割

Segment Everything Everywhere All at Once（https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once）：支持使用各种类型的提示（文本、点、涂鸦、引用图像等）和提示的任意组合进行分割图像分割

LLaVA（https://llava-vl.github.io/）：使用gpt4进行视觉指令调整的视觉语言模型

Automated Dataset Annotation and Evaluation with GroundingDINO and SAM（https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb）：自动数据生成，生成可供训练的目标检测和实例分割数据集

X-Decoder（https://github.com/microsoft/X-Decoder）：可以生成像素级分割和token级文本

## 4.点云：

VoxelNeXt（https://github.com/dvlab-research/VoxelNeXt）：3d目标检测和追踪模型

## 5.其他

Visual ChatGPT （https://github.com/microsoft/TaskMatrix）：是一个链接chatgpt的工具，可以在聊天中上传和接收图片